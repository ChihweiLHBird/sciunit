{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![SciUnit Logo](https://raw.githubusercontent.com/scidash/assets/master/logos/sciunit.png)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ChihweiLHBird/sciunit/blob/dev/docs/QuickTutorialsNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Chapter 1. What is SciUnit?\n",
    "\n",
    "### SciUnit is a framework for validating scientific models by creating experimental-data-driven unit tests.  \n",
    "\n",
    "### Everyone hopes that their model has some correspondence with reality.  Usually, checking whether this is true is done informally.\n",
    "### SciUnit makes this formal and transparent.  \n",
    "\n",
    "The figure below illustrates both the results you get (center table) and the relationship between the components listed here. \n",
    "\n",
    "![Cosmo Example](https://raw.githubusercontent.com/scidash/assets/master/figures/cosmo_example.png)\n",
    "\n",
    "# Chapter 2. Writing a `model` and `test` in SciUnit from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the follwing line if ruuning this notebook on Google Colab\n",
    "!pip install -q sciunit\n",
    "\n",
    "import sciunit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciUnit works by making models declare and implement capabilities that tests use to interact with those models.  \n",
    "Each `capability` is a subclass of `sciunit.Capability`, and contains one or more unimplemented methods.  Here we define a simple capability through which a model can return a single number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProducesNumber(sciunit.Capability):\n",
    "    \"\"\"An example capability for producing some generic number.\"\"\"\n",
    "\n",
    "    def produce_number(self):\n",
    "        \"\"\"The implementation of this method should return a number.\"\"\"\n",
    "        raise NotImplementedError(\"Must implement produce_number.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciUnit models subclass `sciunit.Model` as well as each `sciunit.Capability` they aim to implement. \n",
    "Here we create a trivial model class that is instantiated with a single constant.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciunit.capabilities import ProducesNumber # One of many potential model capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstModel(sciunit.Model, \n",
    "                 ProducesNumber):\n",
    "    \"\"\"A model that always produces a constant number as output.\"\"\"\n",
    "    \n",
    "    def __init__(self, constant, name=None):\n",
    "        self.constant = constant \n",
    "        super(ConstModel, self).__init__(name=name, constant=constant)\n",
    "\n",
    "    def produce_number(self):\n",
    "        return self.constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A `model` we want to test is always an instance (with specific model arguments) of a more generic `model` class.  \n",
    "Here we create an instance of `ConstModel` that will always produce the number 37 and give it a name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_model_37 = ConstModel(37, name=\"Constant Model 37\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A SciUnit test class must contain:\n",
    "1. the capabilities a model requires to take the test.  \n",
    "2. the type of score that it will return\n",
    "3. an implementation of `generate_prediction`, which will use the model's capabilities to get some values out of the model.\n",
    "4. an implementaiton of `compute_score`, to use the provided observation and the generated prediction to compute a sciunit `Score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciunit.scores import BooleanScore # One of several SciUnit score types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualsTest(sciunit.Test):\n",
    "    \"\"\"Tests if the model predicts \n",
    "    the same number as the observation.\"\"\"   \n",
    "    \n",
    "    required_capabilities = (ProducesNumber,) # The one capability required for a model to take this test.  \n",
    "    score_type = BooleanScore # This test's 'judge' method will return a BooleanScore.  \n",
    "    \n",
    "    def generate_prediction(self, model):\n",
    "        return model.produce_number() # The model has this method if it inherits from the 'ProducesNumber' capability.\n",
    "    \n",
    "    def compute_score(self, observation, prediction):\n",
    "        score = self.score_type(observation['value'] == prediction) # Returns a BooleanScore. \n",
    "        score.description = 'Passing score if the prediction equals the observation'\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A SciUnit test is a specific instance of a `test` class, parameterized by the observation (i.e. the empirical data that the `model` aims to recapitulate).  \n",
    "Here we create a test instance parameterized by the observation 37.0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "equals_37_test = EqualsTest({'value':37}, name='=37')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every test has a `judge` method which executes the test and returns a `score` for the provide model.  \n",
    "Here we judge the model we just created using the test we just created.  The `judge` method does a lot of things behind the scenes:  \n",
    "1. It checks to makes sure that your `model` expresses each `capability` required to take the test. It doesn't check to see if they are implemented correctly (how could it know?) but it does check to make sure the `model` at least claims (through inheritance) to express each `capability`. The required capabilities are none other than those in the test's `required_capabilities` attribute. Since `ProducesNumber` is the only required capability, and the `ConstModel` class inherits from the corresponding capability class, that check passes.\n",
    "2. It calls the test's `generate_prediction` method, which uses the model's capabilities to make the model return some quantity of interest, in this case a characteristic number.\n",
    "3. It calls the test's `compute_score` method, which compares the observation the test was instantiated with against the prediction returned in the previous step. This comparison of quantities is cast into a score (in this case, a `BooleanScore`), bound to some `model` output of interest (in this case, the number produces by the `model`), and that `score` object is returned.\n",
    "4. The `score` returned is checked to make sure it is of the type promised in the class definition, i.e. that a `BooleanScore` is returned if a `BooleanScore` is listed in the `score_type` attribute of the `test`.\n",
    "5. The `score` is bound to the `test` that returned it, the `model` that took the `test`, and the prediction and observation that were used to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = equals_37_test.judge(const_model_37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A score is an object containing information about the result of the test, and the provenance of that result.  \n",
    "Printing the `score` just prints a representation of its value (for a `BooleanScore`, `True` has the representation 'Pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pass"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also summarize the `score` in its entirety, printing information about the associated `model` and `test`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "=== Model Constant Model 37 achieved score Pass on test '=37'. ===\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How was that score computed again?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Passing score if the prediction equals the observation\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Several logically related tests can be grouped using a `TestSuite`.  \n",
    "These can be instances of the same test class (instantiated with different observations) or instances of different test classes.  Anything tests that you think belongs together can be part of a TestSuite.  A test can be a part of many different suites at once.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "equals_1_test = EqualsTest({'value':1}, name='=1') # Test that model output equals 1.  \n",
    "equals_2_test = EqualsTest({'value':2}, name='=2') # Test that model output equals 2.  \n",
    "\n",
    "equals_suite = sciunit.TestSuite([equals_1_test, equals_2_test, equals_37_test], name=\"Equals test suite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our model using this TestSuite, and display the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Executing test <i>=1</i> on model <i>Constant Model 37</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(230,78,52)\">Fail</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>=2</i> on model <i>Constant Model 37</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(230,78,52)\">Fail</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>=37</i> on model <i>Constant Model 37</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(60,169,88)\">Pass</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>=1</th>\n",
       "      <th>=2</th>\n",
       "      <th>=37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Constant Model 37</th>\n",
       "      <td>Fail</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     =1    =2   =37\n",
       "Constant Model 37  Fail  Fail  Pass"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_matrix = equals_suite.judge(const_model_37)\n",
    "score_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create more models and subject those to the test suite to get a more extensive score matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Executing test <i>=1</i> on model <i>Constant Model 1</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(60,169,88)\">Pass</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>=2</i> on model <i>Constant Model 1</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(230,78,52)\">Fail</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>=37</i> on model <i>Constant Model 1</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(230,78,52)\">Fail</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>=1</i> on model <i>Constant Model 2</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(230,78,52)\">Fail</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>=2</i> on model <i>Constant Model 2</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(60,169,88)\">Pass</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>=37</i> on model <i>Constant Model 2</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(230,78,52)\">Fail</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>=1</i> on model <i>Constant Model 37</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(230,78,52)\">Fail</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>=2</i> on model <i>Constant Model 37</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(230,78,52)\">Fail</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>=37</i> on model <i>Constant Model 37</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(60,169,88)\">Pass</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>=1</th>\n",
       "      <th>=2</th>\n",
       "      <th>=37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Constant Model 1</th>\n",
       "      <td>Pass</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Constant Model 2</th>\n",
       "      <td>Fail</td>\n",
       "      <td>Pass</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Constant Model 37</th>\n",
       "      <td>Fail</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     =1    =2   =37\n",
       "Constant Model 1   Pass  Fail  Fail\n",
       "Constant Model 2   Fail  Pass  Fail\n",
       "Constant Model 37  Fail  Fail  Pass"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_model_1 = ConstModel(1, name='Constant Model 1')\n",
    "const_model_2 = ConstModel(2, name='Constant Model 2')\n",
    "score_matrix = equals_suite.judge([const_model_1, const_model_2, const_model_37])\n",
    "score_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the results only for one of the tests in the suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Constant Model 1     Pass\n",
       "Constant Model 2     Fail\n",
       "Constant Model 37    Fail\n",
       "Name: =1, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_matrix[equals_1_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or examine the results only for one of the models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=1     Fail\n",
       "=2     Pass\n",
       "=37    Fail\n",
       "Name: Constant Model 2, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_matrix[const_model_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Testing with help from the SciUnit standard library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sciunit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this chapter we will use the same toy model in Chapter 1 but write a more interesting test with additional features included in SciUnit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciunit.models.examples import ConstModel # One of many dummy models included for illustration.  \n",
    "const_model_37 = ConstModel(37, name=\"Constant Model 37\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a test that validates the `observation` and returns more informative `score` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciunit.capabilities import ProducesNumber\n",
    "from sciunit.scores import ZScore # One of many SciUnit score types.  \n",
    "from sciunit.errors import ObservationError # An exception class raised when a test is instantiated \n",
    "                                     # with an invalid observation.\n",
    "    \n",
    "class MeanTest(sciunit.Test):\n",
    "    \"\"\"Tests if the model predicts \n",
    "    the same number as the observation.\"\"\"   \n",
    "    \n",
    "    required_capabilities = (ProducesNumber,) # The one capability required for a model to take this test.  \n",
    "    score_type = ZScore # This test's 'judge' method will return a BooleanScore.  \n",
    "    \n",
    "    def validate_observation(self, observation):\n",
    "        if type(observation) is not dict:\n",
    "            raise ObservationError(\"Observation must be a python dictionary\")\n",
    "        if 'mean' not in observation:\n",
    "            raise ObservationError(\"Observation must contain a 'mean' entry\")\n",
    "        \n",
    "    def generate_prediction(self, model):\n",
    "        return model.produce_number() # The model has this method if it inherits from the 'ProducesNumber' capability.\n",
    "    \n",
    "    def compute_score(self, observation, prediction):\n",
    "        score = ZScore.compute(observation,prediction) # Compute and return a ZScore object.  \n",
    "        score.description = (\"A z-score corresponding to the normalized location of the observation \"\n",
    "                             \"relative to the predicted distribution.\")\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done two new things here:\n",
    "- The optional `validate_observation` method checks the `observation` to make sure that it is the right type, that it has the right attributes, etc.  This can be used to ensures that the `observation` is exactly as the other core test methods expect.  If we don't provide the right kind of observation:\n",
    "```python\n",
    "-> mean_37_test = MeanTest(37, name='=37')\n",
    "ObservationError: Observation must be a python dictionary\n",
    "```\n",
    "then we get an error.  In contrast, this is what our test was looking for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = {'mean':37.8, 'std':2.1}\n",
    "mean_37_test = MeanTest(observation, name='=37')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of returning a `BooleanScore`, encoding a `True`/`False` value, we return a `ZScore` encoding a more quantitative summary of the relationship between the observation and the prediction.  When we execute the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mean_37_test.judge(const_model_37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Example of RunnableModel and Backend\n",
    "\n",
    "Beside the usual model in previous sections, let’s create a model that run a Backend instance to simulate and obtain results.\n",
    "\n",
    "Firstly, import necessary components from SciUnit package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sciunit, random\n",
    "from sciunit import Test\n",
    "from sciunit.capabilities import Runnable\n",
    "from sciunit.scores import BooleanScore\n",
    "from sciunit.models import RunnableModel\n",
    "from sciunit.models.backends import register_backends, Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define subclasses of SciUnit Backend, Test, and Model.\n",
    "\n",
    "Note:\n",
    "1. A SciUnit Backend subclass should implement ``_backend_run`` method. \n",
    "2. A SciUnit Backend subclass should implement ``run`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNumBackend(Backend):\n",
    "    '''generate a random integer between min and max'''\n",
    "\n",
    "    def set_run_params(self, **run_params):\n",
    "\n",
    "        # get min from run_params, if not exist, then 0.\n",
    "        self.min = run_params.get('min', 0)\n",
    "\n",
    "        # get max from run_params, if not exist, then self.min + 100.\n",
    "        self.max = run_params.get('max', self.min + 100)\n",
    "\n",
    "    def _backend_run(self):\n",
    "        # generate and return random integer between min and max.\n",
    "        return random.randint(self.min, self.max)\n",
    "\n",
    "class RandomNumModel(RunnableModel):\n",
    "    \"\"\"A model that always produces a constant number as output.\"\"\"\n",
    "\n",
    "    def run(self):\n",
    "        self.results = self._backend.backend_run()\n",
    "\n",
    "\n",
    "class RangeTest(Test):\n",
    "    \"\"\"Tests if the model predicts the same number as the observation.\"\"\"\n",
    "\n",
    "    # Default Runnable Capability for RunnableModel\n",
    "    required_capabilities = (Runnable,)\n",
    "\n",
    "    # This test's 'judge' method will return a BooleanScore.\n",
    "    score_type = BooleanScore\n",
    "\n",
    "    def generate_prediction(self, model):\n",
    "        model.run()\n",
    "        return model.results\n",
    "\n",
    "    def compute_score(self, observation, prediction):\n",
    "        score = BooleanScore(\n",
    "            observation['min'] <= prediction and observation['max'] >= prediction\n",
    "        )\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define the model instance named ``model 1.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomNumModel(\"model 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must register any backend isntance in order to use it in model instances.\n",
    "\n",
    "``set_backend`` and ``set_run_params`` methods can help us to set the run-parameters in the model and its backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_backends({\"Random Number\": RandomNumBackend})\n",
    "model.set_backend(\"Random Number\")\n",
    "model.set_run_params(min=1, max=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an observation that requires the generated random integer between 1 and 10 and a test instance that use the observation and against the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = {'min': 1, 'max': 10}\n",
    "oneToTenTest = RangeTest(observation, \"test 1\")\n",
    "score = oneToTenTest.judge(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the score, and we can see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}